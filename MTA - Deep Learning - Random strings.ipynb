{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers.core import Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataFactory:\n",
    "    def  __init__(self, goodvaluespath, alphabet=string.ascii_letters+string.digits+\" \"):\n",
    "        self.valid = []\n",
    "        self.invalid = []\n",
    "        self.alphabet = alphabet\n",
    "\n",
    "        if not os.path.exists(goodvaluespath):\n",
    "            raise ValueError(\"goodvaluespathis not a valid path\")\n",
    "\n",
    "        filenames = os.listdir(goodvaluespath)\n",
    "        for filename in filenames:\n",
    "            currfullpath = os.path.join(goodvaluespath, filename)\n",
    "            with open(currfullpath, \"r\") as fp:\n",
    "                for line in fp:\n",
    "                    cleanedline = self.cleanline(line)\n",
    "                    if cleanedline not in self.valid:\n",
    "                        self.valid.append( cleanedline )\n",
    "\n",
    "        self.generateinvalids()\n",
    "\n",
    "\n",
    "    def cleanline(self, rawinput):\n",
    "        temp = rawinput.strip()\n",
    "        return temp\n",
    "\n",
    "    def getvalid(self):\n",
    "        return self.valid.copy()\n",
    "\n",
    "\n",
    "    def getinvalid(self):\n",
    "        return self.invalid.copy()\n",
    "\n",
    "    def generateinvalids(self):\n",
    "        for goodword in self.valid:\n",
    "            badword = \"\".join([random.choice(self.alphabet) for ch in goodword])\n",
    "            while badword in self.invalid:\n",
    "                badword = \"\".join([random.choice(self.alphabet) for ch in goodword])\n",
    "\n",
    "            self.invalid.append(badword)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## short helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe():\n",
    "    factory = DataFactory(\"StreetNames/\", alphabet=string.ascii_letters + string.digits + \" \")\n",
    "    validDF = pd.DataFrame({'word': factory.getvalid()})\n",
    "    validDF['tag'] = 0\n",
    "    invalidDF = pd.DataFrame({'word': factory.getinvalid()})\n",
    "    invalidDF['tag'] = 1\n",
    "    ret = pd.DataFrame( validDF )\n",
    "    ret = ret.append( invalidDF, ignore_index=True )\n",
    "    ret = ret.sample(frac=1)\n",
    "    ret = ret.reset_index(drop=True)\n",
    "    return ret\n",
    "\n",
    "\n",
    "\n",
    "def statistics(predicitions, targets):\n",
    "    stats = {'TP' :0 ,'FP' :0 ,'TN' :0 ,'FN' :0}\n",
    "    for i in range(len(predicitions)):\n",
    "        isBad    = targets[i, 1] > 0.5\n",
    "        isTagged = predicitions[i ,1] > 0.5\n",
    "        if isBad and isTagged:\n",
    "            stats['TP'] += 1\n",
    "        elif (isBad) and (not isTagged):\n",
    "            stats['FN' ]+=1\n",
    "        elif (not isBad) and (isTagged):\n",
    "            stats['FP' ]+=1\n",
    "        elif (not isBad) and (not isTagged):\n",
    "            stats['TN' ]+=1\n",
    "\n",
    "    stats['PR'] = (1.00000000 *stats['TP']) / (stats['FP'] +stats['TP'] +1)\n",
    "    stats['RE'] = (1.00000000 *stats['TP']) / (stats['FN'] +stats['TP'] +1)\n",
    "    return stats\n",
    "\n",
    "\n",
    "def calculate_precision_cutoff(pred_float, tag):\n",
    "    n_samples = pred_float.shape[0]\n",
    "    cutoffs = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "    cutoff_data = {}\n",
    "    for cutoff in cutoffs:\n",
    "        cutoff_data[cutoff] = {\"TP\":0.0, \"FP\":0.0, \"TN\":0.0, \"FN\":0.0 , \"TOTAL\":n_samples}\n",
    "        for i in range(n_samples):\n",
    "            if (pred_float[i, 0] > cutoff) and (tag[i] ==1):\n",
    "                cutoff_data[cutoff][\"TP\"] += 1.0\n",
    "            elif (pred_float[i, 0] > cutoff) and (tag[i] == 0):\n",
    "                cutoff_data[cutoff][\"FP\"] += 1.0\n",
    "            elif (pred_float[i, 0] <= cutoff) and (tag[i] == 1):\n",
    "                cutoff_data[cutoff][\"FN\"] += 1.0\n",
    "            elif (pred_float[i, 0] <= cutoff) and (tag[i] == 0):\n",
    "                cutoff_data[cutoff][\"TN\"] += 1.0\n",
    "\n",
    "        try:\n",
    "            cutoff_data[cutoff][\"PR\"] = cutoff_data[cutoff][\"TP\"] / (cutoff_data[cutoff][\"TP\"] + cutoff_data[cutoff][\"FP\"])\n",
    "        except ZeroDivisionError:\n",
    "            cutoff_data[cutoff][\"PR\"] = 0\n",
    "        try:\n",
    "            cutoff_data[cutoff][\"RE\"] = cutoff_data[cutoff][\"TP\"] / (cutoff_data[cutoff][\"TP\"] + cutoff_data[cutoff][\"FN\"])\n",
    "        except ZeroDivisionError:\n",
    "            cutoff_data[cutoff][\"RE\"] = 0\n",
    "        try:\n",
    "            cutoff_data[cutoff][\"AC\"] = (cutoff_data[cutoff][\"TP\"]+cutoff_data[cutoff][\"TN\"]) / (n_samples)\n",
    "        except ZeroDivisionError:\n",
    "            cutoff_data[cutoff][\"AC\"] = 0\n",
    "\n",
    "    return cutoff_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# charcter based histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_histogram(df):\n",
    "    histogram = {}\n",
    "    #df = pd.DataFrame()\n",
    "    i = 0\n",
    "    for row in df[ df['tag']==0 ]['word']:\n",
    "        for ch in row:\n",
    "            if not ch in histogram:\n",
    "                histogram[ch] = 0\n",
    "            histogram[ch] += 1\n",
    "\n",
    "    return histogram\n",
    "\n",
    "\n",
    "\n",
    "def get_percentage(histogram):\n",
    "    sum_appearances = sum([v for k, v in histogram.items()])\n",
    "    percentage = {}\n",
    "    for k, v in histogram.items():\n",
    "        percentage[k] = ((1.000000 * v) / sum_appearances)\n",
    "    return percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_df(df, percentage):\n",
    "    for i in range(21):\n",
    "        df[\"ch{0}\".format(i + 1)] = df['word'].str[i]\n",
    "        df[\"pr{0}\".format(i + 1)] = df['word'].str[i].map(percentage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_vec(row, histogram):\n",
    "    ret_val = {}\n",
    "    word = row['word']\n",
    "    for (i, ch1, ch2, ch3) in zip(range(2, len(word)), word[:], word[1:], word[2:]):\n",
    "        key1 = (i, ch1, ch2, ch3)\n",
    "        key2 = ( ch1, ch2 )\n",
    "        mone = ((1.0000000 * histogram['a_and_b'][key1]) / (histogram['a_and_b']['total']))\n",
    "        mechane = ((1.0000000 * histogram['b'][key2]) / (histogram['b']['total']))\n",
    "        ret_val[\"pr_{0}\".format(i)] = mone / mechane\n",
    "\n",
    "    return pd.Series(ret_val)\n",
    "\n",
    "\n",
    "def make_histogram_naive(df):\n",
    "    histogram = {'a_and_b':{'total':0} , 'b': {'total':0} }\n",
    "    for (idx, row) in df.iterrows():\n",
    "        word = row['word']\n",
    "        for (i, ch1, ch2, ch3) in zip(range(2, len(word)), word[:], word[1:], word[2:]):\n",
    "            if (i, ch1, ch2, ch3) not in histogram['a_and_b']:\n",
    "                histogram['a_and_b'][(i, ch1, ch2, ch3)] = 0\n",
    "            histogram['a_and_b'][(i, ch1, ch2, ch3)] += 1\n",
    "            histogram['a_and_b']['total'] += 1\n",
    "\n",
    "        for ( ch1, ch2 ) in zip(word[:], word[1:]):\n",
    "            if (ch1, ch2) not in histogram['b']:\n",
    "                histogram['b'][( ch1, ch2 )] = 0\n",
    "            histogram['b'][(ch1, ch2)] += 1\n",
    "            histogram['b']['total'] += 1\n",
    "    \n",
    "    return histogram\n",
    "    \n",
    "def run_naive_method(df):\n",
    "    \"\"\"\n",
    "\n",
    "    :param df:\n",
    "    :type df: pandas.DataFrame\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    histogram = make_histogram_naive(df)\n",
    "\n",
    "    print (histogram)\n",
    "    df = df.merge( df.apply(make_vec, axis=1, broadcast=False, raw=False, reduce=None, args=(histogram,)),\n",
    "              left_index=True, right_index=True)\n",
    "    df.fillna(0.0000, inplace=True)\n",
    "    wanted_columns = [ 'pr_2', 'pr_3', 'pr_4', 'pr_5', 'pr_6', 'pr_7', 'pr_8', 'pr_9', 'pr_10', 'pr_11',\n",
    "                      'pr_12', 'pr_13', 'pr_14', 'pr_15', 'pr_16', 'pr_17', 'pr_18', 'pr_19', 'pr_20', 'tag']\n",
    "\n",
    "    np_array = df[wanted_columns].values\n",
    "    #split train & test\n",
    "    X = np_array[:, :-1]\n",
    "    Y = np_array[:, -1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.3)\n",
    "    model = LogisticRegression(max_iter=1000, verbose=1)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    predictions = model.decision_function(X_test)\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, predictions)\n",
    "    print (max ( map(lambda x: ( x[0]+x[1], x ) , zip(precision, recall))))\n",
    "    # PRECISION: 0.9132584269662921   RECALL: 0.9819135717244236))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_naive_dnn(new_df):\n",
    "    \"\"\":type new_df: pd.DataFrame \"\"\"\n",
    "\n",
    "    X = new_df[['pr1', 'pr2', 'pr3', 'pr4', 'pr5', 'pr6', 'pr7', 'pr8', 'pr9', 'pr10', 'pr11', 'pr12', 'pr13', 'pr14', 'pr15', 'pr16', 'pr17', 'pr18', 'pr19', 'pr20', 'pr21']].values\n",
    "    Y = new_df['tag'].values\n",
    "\n",
    "    n_samples = X.shape[0]\n",
    "    train_rand_idx = np.random.choice(  range( n_samples ), size=int(0.7*n_samples), replace=False )\n",
    "    test_idx = [x for x in range(n_samples) if x not in train_rand_idx]\n",
    "    train_x = X[train_rand_idx, : ]\n",
    "    train_y = Y[train_rand_idx]\n",
    "    test_x = X[test_idx, :]\n",
    "    test_y = Y[test_idx]\n",
    "\n",
    "    # Set constants\n",
    "    batch_size = 128\n",
    "    dimof_input = 21\n",
    "    dimof_middle = 10\n",
    "    dimof_output = 1\n",
    "    dropout = 0.1\n",
    "\n",
    "    verbose = True\n",
    "    print('batch_size: ', batch_size)\n",
    "    print('dimof_middle: ', dimof_middle)\n",
    "    print('dropout: ', dropout)\n",
    "    #print('countof_epoch: ', countof_epoch)\n",
    "\n",
    "    print('verbose: ', verbose)\n",
    "    print()\n",
    "\n",
    "    # Set model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(dimof_middle, input_dim=dimof_input, init='uniform', activation='sigmoid'))\n",
    "    model.add(Dense(dimof_middle, init='uniform', activation='sigmoid'))\n",
    "    model.add(Dense(dimof_output, init='uniform', activation='sigmoid'))\n",
    "    optimizer = Adam(lr=0.001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])\n",
    "    print (model.summary())\n",
    "    # Train\n",
    "\n",
    "    model.fit(\n",
    "        train_x, train_y,\n",
    "        shuffle=True,\n",
    "        #validation_split=0.2,\n",
    "        batch_size=batch_size, epochs=20, verbose=verbose)\n",
    "\n",
    "    # Test\n",
    "    x = model.predict(test_x)\n",
    "    d = calculate_precision_cutoff(x, test_y)\n",
    "    print (\"\\n\\nRESULTS:\\n========\\n\")\n",
    "    for k in d.keys():\n",
    "        print (\"cutoff: \", k, \" PR: \", d[k][\"PR\"], \" RE: \", d[k][\"RE\"], \" AC: \", d[k][\"AC\"])\n",
    "\n",
    "def run_nn_method(df):\n",
    "    histogram = get_histogram(df)\n",
    "    percentage = get_percentage(histogram)\n",
    "    transform_df(df, percentage)\n",
    "    new_df = df[\n",
    "        ['pr1', 'pr2', 'pr3', 'pr4', 'pr5', 'pr6', 'pr7', 'pr8', 'pr9', 'pr10', 'pr11', 'pr12', 'pr13', 'pr14', 'pr15',\n",
    "         'pr16', 'pr17', 'pr18', 'pr19', 'pr20', 'pr21', 'tag']]\n",
    "    new_df.fillna(value=0, inplace=True)\n",
    "    learn_naive_dnn(new_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ybercovich\\pycharmprojects\\deeplearning-randomstrings\\venv\\lib\\site-packages\\pandas\\core\\frame.py:3035: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  downcast=downcast, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size:  128\n",
      "dimof_middle:  10\n",
      "dropout:  0.1\n",
      "verbose:  True\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 10)                220       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 341\n",
      "Trainable params: 341\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ybercovich\\pycharmprojects\\deeplearning-randomstrings\\venv\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(10, input_dim=21, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "c:\\users\\ybercovich\\pycharmprojects\\deeplearning-randomstrings\\venv\\lib\\site-packages\\ipykernel_launcher.py:34: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(10, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "c:\\users\\ybercovich\\pycharmprojects\\deeplearning-randomstrings\\venv\\lib\\site-packages\\ipykernel_launcher.py:35: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "67492/67492 [==============================] - 1s 15us/step - loss: 0.2500 - acc: 0.5051\n",
      "Epoch 2/20\n",
      "67492/67492 [==============================] - 1s 12us/step - loss: 0.2259 - acc: 0.7283\n",
      "Epoch 3/20\n",
      "67492/67492 [==============================] - 1s 12us/step - loss: 0.0919 - acc: 0.9230\n",
      "Epoch 4/20\n",
      "67492/67492 [==============================] - 1s 13us/step - loss: 0.0533 - acc: 0.9421\n",
      "Epoch 5/20\n",
      "67492/67492 [==============================] - 1s 12us/step - loss: 0.0435 - acc: 0.9492\n",
      "Epoch 6/20\n",
      "67492/67492 [==============================] - 1s 12us/step - loss: 0.0397 - acc: 0.9512\n",
      "Epoch 7/20\n",
      "67492/67492 [==============================] - 1s 12us/step - loss: 0.0380 - acc: 0.9523\n",
      "Epoch 8/20\n",
      "67492/67492 [==============================] - 1s 12us/step - loss: 0.0371 - acc: 0.9526\n",
      "Epoch 9/20\n",
      "67492/67492 [==============================] - 1s 12us/step - loss: 0.0366 - acc: 0.9529\n",
      "Epoch 10/20\n",
      "67492/67492 [==============================] - 1s 12us/step - loss: 0.0363 - acc: 0.9530\n",
      "Epoch 11/20\n",
      "67492/67492 [==============================] - 1s 12us/step - loss: 0.0362 - acc: 0.9532\n",
      "Epoch 12/20\n",
      "67492/67492 [==============================] - 1s 13us/step - loss: 0.0361 - acc: 0.9530\n",
      "Epoch 13/20\n",
      "67492/67492 [==============================] - 1s 12us/step - loss: 0.0360 - acc: 0.9531\n",
      "Epoch 14/20\n",
      "67492/67492 [==============================] - 1s 12us/step - loss: 0.0360 - acc: 0.9531\n",
      "Epoch 15/20\n",
      "67492/67492 [==============================] - 1s 12us/step - loss: 0.0359 - acc: 0.9534\n",
      "Epoch 16/20\n",
      "67492/67492 [==============================] - 1s 12us/step - loss: 0.0359 - acc: 0.9534\n",
      "Epoch 17/20\n",
      "67492/67492 [==============================] - 1s 13us/step - loss: 0.0359 - acc: 0.9535\n",
      "Epoch 18/20\n",
      "67492/67492 [==============================] - 1s 13us/step - loss: 0.0359 - acc: 0.9533\n",
      "Epoch 19/20\n",
      "67492/67492 [==============================] - 1s 12us/step - loss: 0.0360 - acc: 0.9533\n",
      "Epoch 20/20\n",
      "67492/67492 [==============================] - 1s 12us/step - loss: 0.0359 - acc: 0.9532\n",
      "\n",
      "\n",
      "RESULTS:\n",
      "========\n",
      "\n",
      "cutoff:  0.1  PR:  0.8688766114180478  RE:  0.9873046875  AC:  0.9198644817810966\n",
      "cutoff:  0.2  PR:  0.9104952294411631  RE:  0.978515625  AC:  0.9416787665076402\n",
      "cutoff:  0.3  PR:  0.9300147196574334  RE:  0.9695870535714286  AC:  0.9487658162207011\n",
      "cutoff:  0.4  PR:  0.9426319396847156  RE:  0.9593331473214286  AC:  0.9509092166217245\n",
      "cutoff:  0.5  PR:  0.9528711484593837  RE:  0.9491489955357143  AC:  0.9515314941575054\n",
      "cutoff:  0.6  PR:  0.9605978260869565  RE:  0.93701171875  AC:  0.9497338034985826\n",
      "cutoff:  0.7  PR:  0.9692657522188807  RE:  0.9217354910714286  AC:  0.946726128742308\n",
      "cutoff:  0.8  PR:  0.9774264649996199  RE:  0.8970424107142857  AC:  0.9387056627255757\n",
      "cutoff:  0.9  PR:  0.9844992695990911  RE:  0.84619140625  AC:  0.9171679457927124\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = get_dataframe()\n",
    "run_nn_method(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear](1.8966150321516524, (0.9129902750048303, 0.9836247571468221))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = get_dataframe()\n",
    "run_naive_method(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
